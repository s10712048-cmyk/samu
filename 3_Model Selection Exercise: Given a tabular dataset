import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report,
confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# --- Dataset ---
data = {
 'Pregnancies':[6,1,8,1,0,5,3,10,2,8],
 'Glucose':[148,85,183,89,137,116,78,115,197,125],
 'BloodPressure':[72,66,64,66,40,74,50,70,70,96],
 'SkinThickness':[35,29,0,23,35,0,32,30,45,0],
 'Insulin':[0,0,0,94,168,0,88,96,543,0],
 'BMI':[33.6,26.6,23.3,28.1,43.1,25.6,31.0,34.6,30.5,0],

'DiabetesPedigree':[0.627,0.351,0.672,0.167,2.288,0.201,0.248,0.529,0.15
8,0.232],
 'Age':[50,31,32,21,33,30,26,54,53,54],
 'Outcome':[1,0,1,0,1,0,1,0,1,1]
}
df = pd.DataFrame(data)
# Replace zero values with mean
cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
for c in cols:
 df[c] = df[c].replace(0, np.nan).fillna(df[c].mean())
X = df.drop('Outcome', axis=1)
y = df['Outcome']
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=0.2, random_state=42, stratify=y)
# Scaling
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)
# Models
log_reg = LogisticRegression(max_iter=1000)
rf = RandomForestClassifier(random_state=42)
log_reg.fit(X_train_s, y_train)
rf.fit(X_train_s, y_train)
log_pred = log_reg.predict(X_test_s)
rf_pred = rf.predict(X_test_s)
# Evaluation function
def eval_model(name, y_true, y_pred):
 print(f"\n{name} Accuracy: {accuracy_score(y_true, y_pred)}")
 print(classification_report(y_true, y_pred))
 print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
# Evaluate both models
eval_model("Logistic Regression", y_test, log_pred)
eval_model("Random Forest", y_test, rf_pred)
# Feature importance
print("\nLogistic Regression Coefficients:")
print(pd.DataFrame({'Feature': X.columns, 'Coef': log_reg.coef_[0]}))
print("\nRandom Forest Feature Importance:")
print(pd.DataFrame({'Feature': X.columns, 'Importance':
rf.feature_importances_}))










import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler

data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
score_lr = r2_score(y_test, model_lr.predict(X_test))

model_dt = DecisionTreeRegressor(random_state=42)
model_dt.fit(X_train, y_train)
score_dt = r2_score(y_test, model_dt.predict(X_test))

print(f"Linear Regression R2: {score_lr}")
print(f"Decision Tree R2: {score_dt}")

if score_dt > score_lr:
    final_model = model_dt
    print("Selected: Decision Tree")
else:
    final_model = model_lr
    print("Selected: Linear Regression")

sample = X_test[0].reshape(1, -1)
print(f"Prediction: {final_model.predict(sample)[0]}")
